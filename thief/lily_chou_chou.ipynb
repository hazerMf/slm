{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0082cc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: requests in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\djuybu\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b71ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "973d9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://www.lily-chou-chou.jp/holic/bbs/htm/index2_mddl.php?ofst=\"\n",
    "START_PAGES = 427032 #start from 427032th pages back to 1\n",
    "LIMIT = 425900 #upto 427000th pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "547b5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ad26ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== NETWORK =====\n",
    "CONNECT_TIMEOUT = 5\n",
    "READ_TIMEOUT = 120     # offset sÃ¢u cáº§n read timeout dÃ i\n",
    "\n",
    "# ===== SPEED CONTROL =====\n",
    "BASE_SLEEP = 3.0     # tá»‘i thiá»ƒu 10s / request\n",
    "JITTER = 5.0          # random thÃªm 0â€“5s\n",
    "MAX_RETRY = 2         # KHÃ”NG retry nhiá»u\n",
    "\n",
    "# ===== BLOCK CONTROL =====\n",
    "FAIL_THRESHOLD = 3\n",
    "BLOCK_PAUSE = 5 * 60   # 5 phÃºt\n",
    "\n",
    "# ===== WINDOW =====\n",
    "WINDOW_SIZE = 100        # crawl ráº¥t sÃ¢u â†’ window nhá»\n",
    "\n",
    "# ===== ENCODING =====\n",
    "PAGE_ENCODING = \"euc-jp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a727769",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROGRESS_FILE = \"crawl_progress.json\" # File lÆ°u tráº¡ng thÃ¡i offset vÃ  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "658229c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
    "                checkpoint = json.load(f)\n",
    "                print(f\"ðŸ”„ TÃ¬m tháº¥y tiáº¿n trÃ¬nh cÅ©: Offset cuá»‘i cÃ¹ng lÃ  {checkpoint['last_offset']}\")\n",
    "                return checkpoint['last_offset'], checkpoint['post_list']\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Lá»—i Ä‘á»c file checkpoint, báº¯t Ä‘áº§u má»›i: {e}\")\n",
    "    return START_PAGES, []\n",
    "\n",
    "def save_checkpoint(last_offset, post_list):\n",
    "    checkpoint = {\n",
    "        \"last_offset\": last_offset,\n",
    "        \"post_list\": post_list\n",
    "    }\n",
    "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(checkpoint, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"ðŸ’¾ ÄÃ£ lÆ°u checkpoint táº¡i offset {last_offset}. Tá»•ng sá»‘ bÃ i: {len(post_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a20b8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5de9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec4db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_offset(ofst: int) -> bool:\n",
    "    url = BASE_URL + str(ofst)\n",
    "\n",
    "    response = None\n",
    "\n",
    "    for attempt in range(MAX_RETRY + 1):\n",
    "        try:\n",
    "            response = session.get(\n",
    "                url,\n",
    "                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT)\n",
    "            )\n",
    "            break\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            wait = BASE_SLEEP * (attempt + 1)\n",
    "            print(f\"[TIMEOUT] ofst={ofst}, wait {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] ofst={ofst}: {e}\")\n",
    "            return False\n",
    "\n",
    "    if response is None or response.status_code != 200:\n",
    "        print(f\"[SKIP] ofst={ofst}\")\n",
    "        return False\n",
    "\n",
    "    if len(response.content) < 800:\n",
    "        print(f\"[SKIP] tiny response at ofst={ofst}\")\n",
    "        return False\n",
    "\n",
    "    response.encoding = PAGE_ENCODING\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    all_trs = soup.find_all('tr')\n",
    "\n",
    "    current_post = {}\n",
    "\n",
    "    for tr in all_trs:\n",
    "        # ===== HEADER =====\n",
    "        header_td = tr.find('td', attrs={'width': '450', 'colspan': '2'})\n",
    "        if header_td and not header_td.has_attr('align'):\n",
    "            fonts = header_td.find_all('font')\n",
    "            if len(fonts) >= 4:\n",
    "                current_post = {}\n",
    "                current_post['user'] = fonts[1].get_text(strip=True)\n",
    "                current_post['post_name'] = fonts[3].get_text(strip=True)\n",
    "\n",
    "        # ===== CONTENT =====\n",
    "        content_td = tr.find(\n",
    "            'td',\n",
    "            attrs={'width': '450', 'colspan': '2', 'align': 'left'}\n",
    "        )\n",
    "        if content_td and 'user' in current_post:\n",
    "            fonts = content_td.find_all('font')\n",
    "            if fonts:\n",
    "                current_post['post_content'] = fonts[0].get_text(strip=True)\n",
    "                POST_LIST.append(current_post)\n",
    "                current_post = {}\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a3192ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\djuybu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\djuybu\\appdata\\roaming\\python\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce877cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb80e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ TÃ¬m tháº¥y tiáº¿n trÃ¬nh cÅ©: Offset cuá»‘i cÃ¹ng lÃ  426628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Window 426628 â†’ 426528:   0%|          | 0/728 [00:00<?, ?post/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRAWLING] ofst=426628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Window 426628 â†’ 426528:   0%|          | 1/728 [00:11<2:20:12, 11.57s/post]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CRAWLING] ofst=426627\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Táº£i tiáº¿n trÃ¬nh cÅ© náº¿u cÃ³\n",
    "current_start_offset, POST_LIST = load_checkpoint()\n",
    "\n",
    "# 1. TÃ­nh toÃ¡n tá»•ng sá»‘ lÆ°á»£ng cáº§n crawl Ä‘á»ƒ hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh\n",
    "total_to_crawl = current_start_offset - LIMIT\n",
    "consecutive_failures = 0\n",
    "\n",
    "# 2. Khá»Ÿi táº¡o tqdm\n",
    "with tqdm(total=total_to_crawl, desc=\"ðŸš€ Äang chuáº©n bá»‹...\", unit=\"post\") as pbar:\n",
    "    \n",
    "    # Duyá»‡t theo tá»«ng window\n",
    "    for window_start in range(current_start_offset, LIMIT, -WINDOW_SIZE):\n",
    "        window_end = max(window_start - WINDOW_SIZE, LIMIT)\n",
    "        \n",
    "        pbar.set_description(f\"ðŸ“‚ Window {window_start} â†’ {window_end}\")\n",
    "\n",
    "        for ofst in range(window_start, window_end, -1):\n",
    "            ok = crawl_offset(ofst)\n",
    "\n",
    "            if not ok:\n",
    "                consecutive_failures += 1\n",
    "                pbar.write(f\"âš ï¸ [FAIL] Offset {ofst} | LiÃªn tiáº¿p: {consecutive_failures}\")\n",
    "            else:\n",
    "                consecutive_failures = 0\n",
    "\n",
    "            # Cáº­p nháº­t thanh tiáº¿n trÃ¬nh thÃªm 1 Ä‘Æ¡n vá»‹\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Xá»­ lÃ½ khi bá»‹ block\n",
    "            if consecutive_failures >= FAIL_THRESHOLD:\n",
    "                pbar.write(\"\\nðŸš¨ [BLOCK DETECTED] - Äang lÆ°u vÃ  táº¡m nghá»‰...\")\n",
    "                save_checkpoint(ofst, POST_LIST) \n",
    "                \n",
    "                # Cáº­p nháº­t tráº¡ng thÃ¡i nghá»‰ lÃªn thanh tiáº¿n trÃ¬nh\n",
    "                for i in range(BLOCK_PAUSE, 0, -1):\n",
    "                    pbar.set_description(f\"â¸ Nghá»‰ do Block ({i}s)\")\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                consecutive_failures = 0\n",
    "                break # ThoÃ¡t vÃ²ng láº·p offset hiá»‡n táº¡i\n",
    "\n",
    "            # Sleep ngáº¯n giá»¯a cÃ¡c request\n",
    "            sleep_time = BASE_SLEEP + random.random() * JITTER\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        # SAU Má»–I WINDOW\n",
    "        save_checkpoint(window_end, POST_LIST)\n",
    "        pbar.write(f\"âœ… ÄÃ£ xong window tá»›i offset {window_end}. Tá»•ng: {len(POST_LIST)}\")\n",
    "        \n",
    "        # Nghá»‰ dÃ i giá»¯a cÃ¡c window (trá»« khi Ä‘Ã£ háº¿t dá»¯ liá»‡u)\n",
    "        if window_end > LIMIT:\n",
    "            for i in range(BLOCK_PAUSE, 0, -1):\n",
    "                time.sleep(1)\n",
    "\n",
    "pbar.write(f\"ðŸŽ‰ [DONE] HoÃ n táº¥t crawl. Tá»•ng sá»‘ bÃ i: {len(POST_LIST)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e32668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lily_chou_chou_posts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(POST_LIST, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
